# 学习总结

## 微服务

1、服务发现——Netflix Eurek

该系统下还分为Eureka服务端和Eureka客户端，Eureka服务端用作服务注册中心，支持集群部署。Eureka客户端是一个java客户端，用来处理服务注册与发现

2、客服端负载均衡——Netflix Ribbon

基于Http和Tcp的客户端负载均衡，使得面向REST请求时变换为客户端的负载服务调用，提供客户端的软件负载均衡算法。

3、断路器——Netflix Hystrix

它的作用是保护系统，控制故障范围。

1. 服务降级(fallback)

服务器忙，请稍后再试，不让客户端等待并立刻返回一个友好提示，fallback

哪些情况会出发降级

程序运行导常
超时
服务熔断触发服务降级
线程池/信号量打满也会导致服务降级
2. 服务熔断(break)

类比保险丝达到最大服务访问后，直接拒绝访问，拉闸限电，然后调用服务降级的方法并返回友好提示。

服务的降级 -> 进而熔断 -> 恢复调用链路

3. 服务限流(flowlimit)

秒杀高并发等操作，严禁一窝蜂的过来拥挤，大家排队，一秒钟N个，有序进行。


4、服务网关——**Spring Cloud Gateway** 

提供api网关，路由，负载均衡等作用；API 网关是一个搭建在客户端和微服务之间的服务，我们可以在 API 网关中处理一些非业务功能的逻辑，例如权限验证、监控、缓存、请求路由等 

网关最基本的模块。它由一个 ID、一个目标 URI、一组断言（Predicate）和一组过滤器（Filter）组成。 

```
filter里数据量大偶发乱码问题解决：
public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
 DataBufferFactory bufferFactory = originalResponse.bufferFactory();
        ServerHttpResponseDecorator decoratedResponse = new ServerHttpResponseDecorator(originalResponse) {
            @Override
            public Mono<Void> writeWith(Publisher<? extends DataBuffer> body) {
             if (body instanceof Flux) {
                    Flux<? extends DataBuffer> fluxBody = (Flux<? extends DataBuffer>) body;
                    return super.writeWith(fluxBody.buffer().map(dataBuffers -> {//解决返回体分段传输
                    
                    //如果响应过大，会进行截断，出现乱码，然后看api DefaultDataBufferFactory有个join方法可以合并所有的流，乱码的问题解决
                        DataBufferFactory dataBufferFactory = new DefaultDataBufferFactory();
                        DataBuffer join = dataBufferFactory.join(dataBuffers);
                        byte[] content = new byte[join.readableByteCount()];
                        join.read(content);
                        //释放掉内存
                        DataBufferUtils.release(join);
                        //组装apiLog对象，用于记录api请求日志
                        String result = new String(content, StandardCharsets.UTF_8);
                        //---end
```

5、分布式配置——Spring Cloud Config

提供服务端和客户端，服务器存储后端的默认实现使用git

Spring Cloud Feign 是一个声明web服务客户端，这使得编写Web服务客户端更容易，使用Feign 创建一个接口并对它进行注解 

Zipkin是一个分布式跟踪系统。它有助于收集解决服务架构中延迟问题所需的计时数据。特性包括此数据的收集和查找。 

 Spring Cloud Sleuth：分布式跟踪实现， 可以完美整合Zipkin。  

**Spring Cloud Security** 



**ES=elaticsearch** **分布式全文检索引擎** 

## MQ

[消息队列](https://so.csdn.net/so/search?q=%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97&spm=1001.2101.3001.7020)中间件是分布式系统中重要的组件，主要解决异步消息，应用解耦，流量削锋和消息通讯四个场景等问题，实现高性能，高可用，可伸缩和最终一致性架构。目前使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ 

1.异步消息：用户注册后，异步发注册邮件和注册短信 

2.应用解耦：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口 ；假如库存系统无法访问，则订单减库存将失败，从而导致订单失败，订单系统与库存系统耦合 

订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功 库存系统：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作 

3.流量削锋：秒杀或团抢活动中 ，用户的请求，服务器接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面。

4.日志处理是指将消息队列用在日志处理中，比如[Kafka](https://so.csdn.net/so/search?q=Kafka&spm=1001.2101.3001.7020)的应用，解决大量日志传输的问题。架构简化如下 



**RocketMQ(阿里开源)写入性能上不如kafka, 主要因为kafka主要应用于日志场景，而RocketMQ应用于业务场景，为了保证消息必达牺牲了性能，且基于线上真实场景没有在RocketMQ层做消息合并，推荐在业务层自己做。** 

RocketMQ是消息中间件，kafka是分布式流式系统，提供了可靠的顺序保证 。 

Kafka 为每个主题维护一个消息分区日志。每个分区都是由有序的不可变的记录序列组成，并且消息都是连续的被追加在尾部。 

生产者可以向一个具体的主题发送消息，然后多个消费者组可以消费相同的消息。每一个消费者组都可以独立的伸缩去处理相应的负载。由于消费者维护自己的分区偏移，所以他们可以选择持久订阅或者临时订阅，持久订阅在重启之后不会丢失偏移而临时订阅在重启之后会丢失偏移并且每次重启之后都会从分区中最新的记录开始读取。 

##### 如何保证消息不丢失

RocketMQ
一、Producer保证消息不丢失

1、RocketMQ发送消息有三种模式，即同步发送，异步发送、单向发送。

同步发送消息时会同步阻塞等待Broker返回发送结果，如果发送失败不会收到发送结果SendResult,这种是最可靠的发送方式。
异步发送消息可以在回调方法中得知发送结果。
单向发送是消息发送完之后就不管了，不管发送成功没成功，是最不可靠的一种方式 

生产者的重试机制 

mq为生产者提供了失败重试机制，同步发送和异步发送默认都是失败重试两次当然可以修改重试次数，如果多次还是失败，那么可以采取记录这条信息，然后人工采取补偿机制。 

**二、Broker保证消息不丢失**

1、刷盘策略

RocketMq持久化消息有两种策略即同步刷盘和异步刷盘。默认情况下是异步刷盘，此模式下当生产者把消息发送到broker，消息存到内存之后就认为消息发送成功了，就会返回给生产者消息发送成功的结果。但是如果消息还没持久化到硬盘，服务器宕机了，那么消息就会丢失。同步刷盘是当Broker接收到消息并且持久化到硬盘之后才会返回消息发送成功的结果，这样就会保证消息不会丢失，但是同步刷盘相对于异步刷盘来说效率上有所降低，大概降低10% 

集群模式

rocketmq的集群模式保证可rocketMQ高可用。利用多master多slave节点保证rocketmq的高可用；此模式是broker保证消息不丢失的配置，主从复制同步复制，刷盘模式同步刷盘，但是这种模式下性能会有所降低 

三、**Consumer保证消息不丢失** 

1、手动ack    业务成功后才返回状态 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS 

## Mybatis

#### 工作原理



## 缓存

##### 穿透 击穿 雪崩

1、缓存穿透
缓存穿透：指在redis缓存中不存在数据，这个时候只能去访问持久层数据库，当用户很多时，缓存都没有命中就会照成很大压力
解决方案 ：
（1）布隆过滤器（对可能查询的数据先用hash存储）
（2）缓存空对象：在没有的数据中存一个空，而这些空的对象会设置一个有效期）

2、缓存击穿
缓存击穿：指在同一个时间内访问一个请求的请求数过多，而在这个时候缓存某个key失效了，这个时候就会冲向数据库照成缓存击穿
解决方案：
（1）设置缓存永远不过期
（2）加互斥锁，使用分布式锁，保证每个key只有一个线程去查询后端服务，而其他线程为等待状态。这种模式将压力转到了分布式锁上

3、缓存雪崩
缓存雪崩：在某个时间段，缓存集体过期、redis宕机、增加删除节点导致
解决方案：给key的失效时间设置为随机时间，避免集体过期；双缓存；加互斥锁

原文链接：https://blog.csdn.net/weixin_44975592/article/details/126287594





## Mysql

##### B+树

![img](https://img-blog.csdnimg.cn/img_convert/398aa23eb83e3170d7a41da31b2c8ddd.png) 

1、B+树的所有数据都存储在叶子节点，非叶子节点只存储索引。

2、叶子节点中的数据使用双向链表的方式进行关联。

2、原因分析
我认为，MySQL索引结构采用B+树，有以下4个原因：

1、从磁盘I/O效率方面来看：B+树的非叶子节点不存储数据，所以树的每一层就能够存储更多的索引数量，也就是说，B+树在层高相同的情况下，比B树的存储数据量更多，间接会减少磁盘I/O的次数。

2、从范围查询效率方面来看：在MySQL中，范围查询是一个比较常用的操作，而B+树的所有存储在叶子节点的数据使用了双向链表来关联，所以B+树在查询的时候只需查两个节点进行遍历就行，而B树需要获取所有节点，因此，B+树在范围查询上效率更高。

3、从全表扫描方面来看：因为，B+树的叶子节点存储所有数据，所以B+树的全局扫描能力更强一些，因为它只需要扫描叶子节点。而B树需要遍历整个树。

4、从自增ID方面来看：基于B+树的这样一种数据结构，如果采用自增的整型数据作为主键，还能更好的避免增加数据的时候，带来叶子节点分裂导致的大量运算的问题。
————————————————
原文链接：https://blog.csdn.net/gupaoedu_tom/article/details/125018395

##### 联合索引的底层结构

**必须有第一个索引字段存在才走索引，可以=也可以范围。**

![在这里插入图片描述](https://img-blog.csdnimg.cn/5f5abd674df046edad4832eab77679a2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAbGl1X3NoaV9qdW4=,size_20,color_FFFFFF,t_70,g_se,x_16) 

索引长度计算方法
计算规则
1.索引字段，没有设置NOT NULL，需要占用一个字节。NULL在mysql中是用一个标志位来表示的，用一个字节,null也走索引，并且排在索引的最前面。并不是有些说法说的那样不走索引的
2.定长字段：tinyiny占1个字节、int占4个字节、bitint占8个字节、date占3个字节、datetime占5个字节，char(n)占n个字符。
3.变长字段：varchar(n)占n个字符+2个字节。
4.不同的字符集，一个字符占用的字节数不同：

latin1编码，每个字符占用一个字节
gbk编码，每个字符占用两个字节
utf8编码，每个字符占用三个字节
utf8mb4编码，每个字符占用四个字节

explain select  * from city where  population = 731200  and name = '3' and district = '12'

像上面的例子，我用的是utf8编码，并且没有设置不可为null。
name索引长度： 2553+2+1=768 （null标志位占一个字节）
age索引长度：4+1=5 （null标志位占一个字节）
position索引长度： 2553+2+1=768 （null标志位占一个字节）
————————————————
原文链接：https://blog.csdn.net/liu_shi_jun/article/details/123132731

不走索引：列做了显性或隐性的类型转换、使用函数、使用'%**'开头、没有使用第一个列、使用了！=或is not null 、范围查询会导致其后边的列不走索引、某列没出现会导致它及以后的都不走索引

**在MySQL中，支持两种排序方式，分别是FileSort和Index排序。**

- Index排序中，索引可以保证数据的有序性，不需要再进行排序，效率更好。

- FileSort排序则一般在内存中进行排序，占用CPU较多。如果待排结果较大，会产生临时文件IO到磁盘进行排序的情况，效率较低。

  order by 不走索引问题：1.强制索引 FORCE INDEX（key） 2.联合索引

##### 回表的含义

**根据非主键索引查询到的结果并没有查找的字段值，此时就需要再次根据主键从聚簇索引（主键索引）的根节点开始查找，这样再次查找到的记录才是完成的。** 

如果查询结果中只是索引的字段，则不需要回表。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200616160842582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ptZW1vcnlz,size_16,color_FFFFFF,t_70) 



 为什么非主键索引结构叶子节点存储的是主键值？
     一是保证一致性，更新数据的时候只需要更新主键索引树，
     二是节省存储空间。
 为什么推荐InnoDB表必须有主键？
     保证会有主键索引树的存在（因为数据存放在主键索引树上面），如果没有mysql会自己生成一个rowid作为自增的主键主键索引
 为什么推荐使用整型的自增主键？
 	一是方便查找比较，
 	二是新增数据的时候只需要在最后加入，不会大规模调整树结构，如果是UUID的话，大小不好比较，新增的时候也极有可能在中间插入数据，会导致树结构大规调整，造成插入数据变慢。

## 分布式

##### 一致性hash

传统求余做[负载均衡](https://so.csdn.net/so/search?q=%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1&spm=1001.2101.3001.7020)算法，缓存节点数由3个变成4个，缓存不命中率为75%。计算方法：穷举hash值为1-12的12个数字分别对3和4取模，然后比较发现只有前3个缓存节点对应结果和之前相同，所以有75%的节点缓存会失效，可能会引起缓存雪崩。 

采用翻倍扩容，避免数据映射全部打乱而全部迁移，翻倍迁移只发生50%的数据迁移 

一致性hash算法 加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。 

数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。所以，出现了增加虚拟节点的方式来减少不均衡的现象。 

![img](https://img-blog.csdnimg.cn/0059405a45674e1899e88826c11139f4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-r5oiR5ouW6Z6L5ZOl,size_20,color_FFFFFF,t_70,g_se,x_16) 

所以采用红黑树是最稳妥的实现方法。Java中直接使用TreeMap即可。 

--https://blog.csdn.net/gonghaiyu/article/details/108375298

##### 幂等性

指的是多次请求接口后不会造成坏的影响，比如重复消费、表单重复提交。

1. 状态机实现幂等：即关注点是状态是否发生变化，若已经是更新后的状态，那么再收到调用请求也不做更新

2. 数据库唯一约束实现幂等

3. 通过tokenid的方式去识别每次请求判断是否重复

   先请求一个token，提交带上，后端比较token，满足后清空token，并执行业务。（防止并发，可以用redis）

   

   最终一致性方式解决

   TCC两阶段补偿方案

   TCC是Try-Conﬁrm-Cancel， 比如在支付场景中，先冻结一笔资金，再去发起支付。如果支付成功，则将冻结资金进行实际扣除；如果支付失败，则取消资金冻结

   - Try阶段：完成所有业务检查（一致性），预留业务资源（准隔离性）
   - Conﬁrm阶段：确认执行业务操作，不做任何业务检查，只使用Try阶段 预留的业务资源。
   - Cancel阶段：取消Try阶段预留的业务资源。Try阶段出现异常时，取消所有业务资源预留请求



##### 分布式锁 

1、处理效率提升：应用分布式锁，可以减少重复任务的执行，避免资源处理效率的浪费；

2、数据准确性保障：使用分布式锁可以放在数据资源的并发访问，避免数据不一致情况，甚至数据损失等

分布式锁的实现前提：
分布式的**CAP理论**：任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。

通常情况下，大家都会牺牲强一致性来换取系统的高可用性，这样我们很多的场景，其实是只需为了保证数据的“最终一致性”。 

**BASE 理论**

（Basically Available）基本可用
在分布式系统出现故障的时候，允许损失部分可用性，即保证核心可用。

（Soft State）软状态
接受一段时间的状态不同步，及中间状态，而改中间状态不影响系统整体可用性。这里的中间状态就是CAP理论中的数据不一致性。

（Eventually Consistent）最终一致性
上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态
————————————————
原文链接：https://blog.csdn.net/universsky2015/article/details/105727244



分布式锁可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。
分布式锁的实现方式有：
 数据库实现分布式锁：原理简单，性能较差：创建一个表，插入记录成功的获得执行权（利用主键唯一性），完成后删除。
 Redis分布式锁：性能最好：【setnx】命令实现分布式锁（set if not exist）
 Zookeeper分布式锁：可靠性最好



##### Hystrix断路器原理及实现（服务降级、熔断、限流）

*熔断*的*实现原理*简单说来就是在一个设定的窗口时间内,根据设置的具体*熔断*策略,判断相应的计数统计是否超过了门限值,如果超过了则会触发*熔断*机制。 

**服务雪崩** ：微服务调用链中某个服务挂了

一、服务降级（fallback）

1、当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的预备响应（FallBack），而不是长时间等待或者抛出调用方无法处理的异常。比如：服务繁忙，请稍后再试，不让客户端等待并立刻返回一个友好提示：fallback。

2、哪些情况会触发降级
（1）程序运行异常
（2）超时
（3）服务熔断触发服务降级
（4）线程池/信号量打满也会导致服务降级

二、服务熔断（break）

1、系统发到最大服务访问量后，直接拒绝访问，限制后续的服务访问，并调用服务降级方法返回友好提示。
2、就是保险丝：服务降级–>进而熔断–>恢复调用链路

三、服务限流（flowlimit）

1、限流的目的是为了保护系统不被大量请求冲垮，通过限制请求的速度来保护系统。在电商的秒杀活动中，限流是必不可少的一个环节。限制高并发，请求进行排队，一秒处理N个请求，有序的进行。
————————————————
原文链接：https://blog.csdn.net/qq_36763419/article/details/120119872

## 高并发

秒杀系统设计

解决高并发的方法主要有：系统拆分，缓存，MQ, 还有分库分表，读写分离等也是分而治之的思想。 

RabbitMq的消息队列除了有解耦和异步的功能外，还可以实现流量削峰 



##### 延时队列，死信队列

生产环境中是不太可能使用JDK原生延迟队列DelayQueue 的，它是**没有持久化**的，重启就会导致数据丢失（也可以：宕机重启，执行任务恢复，从数据库扫描添加回延时队列即可 ） 

1.**有赞**的延迟队列就是基于通过Redis 的`zset`进行设计和存储的 

2.`RabbmitMQ`它的延迟队列机制本质上也是通过`TTL`（Time To Live 消息存活的时间）所实现的，当队列里的元素触发了过期时，会被送往到`Dead Letter Exchanges`（死信队列中)。我们可以将死信队列的元素再次转发，对其进行消费，从而达到延迟队列的效果。 

3.`RocketMQ`的延时等级队列机制 

4.用定时任务 再处理消息也可以



##### 分库分表

数据库的高可用。水平垂直拆分，再到主从复制，读写分离

id如何生成？数据库自增ID、Redis中维护offset、snowflake算法（ twitter 开源的分布式 id 生成算法 ）、基于数据库的号段模式 

- 分库分表，首先得知道瓶颈在哪里，然后才能合理地拆分（分库还是分表？水平还是垂直？分几个？）。且不可为了分库分表而拆分。（比如单表数量量太大则水平分表、单表热点字段太多则垂直分表）

- 选key很重要，既要考虑到拆分均匀，也要考虑到非partition key的查询。

- 只要能满足需求，拆分规则越简单越好。

  实施：大公司用 **mycat** 中间件 Proxy 代理模式 （独立部署） ,小公司用 **（**[`ShardingSphere`](https://github.com/apache/incubator-shardingsphere) **）sharding-jdbc** 客户端，引入jar包

数据分发算法：按照某个字段 Hash ，**缺点** ：扩容比较麻烦 ，需要数据迁移



### 网络

cookie和session

post和put:PUT方法是幂等的，POST方法不是幂等 

1. POST /url 创建
2. PUT /url/xxx 更新

## JDK

java8新特性，四大函数式接口，然后默认方法

##### 注解

注解的本质就是一个继承了 Annotation 接口的接口 

虚拟机将采用 JDK 动态代理机制生成一个目标注解的代理类 ，AnnotationInvocationHandler 中 invoke 方法的实现逻辑，这是核心。一句话概括就是，**通过方法名返回注解属性值** 

##### 序列化

java原生：**实现 java.io.Serializable 接口**  writeObject 和 readObject 方法 ，反射

通过判断类的 serialVersionUID 来验证版本一致性的，如果不一致反序列化会报错；

当某个字段被申明为 transient 后，默认的序列化机制会忽略这个字段

被申明为 transient 的字段，如果需要序列化，可以添加两个私有方法：writeObject 和readObject

其他：

基于XML的SOAP协议及对应的 WebService 框架 

基于 JSON 的简单文本格式编码的 HTTP REST 接口 、Jackson  FastJson  GSON  

对性能和间接性有比较高要求的场景，那么 Hessian、Protobuf、Thrift、Avro 都可以 

##### ThreadLocal 

![img](https://pic2.zhimg.com/80/v2-1bf6db1fff9abebaf6361fba154216ad_720w.webp) 

每个线程都**单独拥有一份共享变量**，这样就可以做到线程之间对于共享变量的隔离问题。 

1.ThreadLocal线程本地变量作为一个Thread类的**成员字段** ：ThreadLocal.ThreadLocalMap threadLocals

key就是ThreadLocal 对象本身，get时先得到线程得到Map，算entry数组的index

首先了解一下 ThreadLocalMap 的 hash 算法 `int i = key.threadLocalHashCode & (len-1)`

`ThreadLocalMap`中`hash`算法很简单，这里`i`就是当前 key 在散列表中对应的数组下标位置。这里最关键的就是`threadLocalHashCode`值的计算，`ThreadLocal`中有一个属性为`HASH_INCREMENT = 0x61c88647`。这个值很特殊，它是**斐波那契数** 也叫 **黄金分割数**。`hash`增量为 这个数字，带来的好处就是 `hash` **分布非常均匀**。

不同的ThreadLocal对象有不同的threadLocalHashCode 值；

（冲突的解决线性探测法 private static int nextIndex(int i, int len) {    return ((i + 1 < len) ? i + 1 : 0);}）

为什么ThreadLocalMap.threshold是2/3而HashMap.threshold是0.75
理解了ThreadLocalMap的数据结构之后可以看出 threshold 越大，则数据存储就会越拥挤，而 threshold 小一点的话就可以减少元素操作时遍历的数量了，这从threadLocalHashCode的设计上也能看出
———————————————— 
原文链接：https://blog.csdn.net/qq_25215821/article/details/126393476

使用场景：

场景一：在重入方法中替代参数的显式传递 ，例如在Spring的@Transaction事务声明的注解中就使用ThreadLocal保存了当前的Connection对象，避免在本次调用的不同方法中使用不同的Connection对象。 

场景二：全局存储用户信息 ，以尝试使用ThreadLocal替代Session的使用 

避免内存泄露 ，每次使用完之后都remove掉Entry ；原因：ThreadLocal 对象==null后，只是map的key被回收了，value还是强引用。

子线程要获得父线程的变量，可以用**InheritableThreadLocal**  

##### 软弱强虚引用

##### HashMap

https://www.cnblogs.com/qcblog/p/8449624.html

某个key散列地址计算过程实际就是: indexFor(hash(key.hashCode()),length)

再hash（叫做**扰动函数**）的目地：防止低位相同高位不同的hash值冲突;

扰动函数：(h = key.hashCode()) ^ (h >>> 16); 

将h的哈希值右移16位并与自身相异或 相当于 使自己的高16位和低16位 相异或，得到的值**既包含了自己高位的特性又包含了自己低位的特性**，从而增加了之后得到的下标的不确定性，降低了碰撞的概率。 



解决哈希冲突办法 ：

1.开放定址法：我们在遇到哈希冲突时，去寻找一个新的空闲的哈希地址。
线性探测法：h(x)=(Hash(x)+i)mod (Hashtable.length);（i会逐渐递增加1）
平方探测法（二次探测）：前后都找
2.再哈希法：同时构造多个不同的哈希函数，等发生哈希冲突时就使用第二个、第三个……等其他的哈希函数计算地址，
直到不发生冲突为止。虽然不易发生聚集，但是增加了计算时间。
3.链地址法：将所有哈希地址相同的记录都链接在同一链表中。
4.建立公共溢出区：将哈希表分为基本表和溢出表，将发生冲突的都存放在溢出表中。

 问题：如果一个类的hashcode都是1，会怎么样？

map.put 会退化为单链表，因为hashcode全冲突的；如果equals 也相同，则put会覆盖掉



为什么用31计算hash？

合适的质数发生哈希冲突的可能性更低 ，31可以被JVM优化 ， 31 * i = (i << 5) - i ，位运算提高效率。



dk1.8 数组 + 链表 + 红黑树

当一个结点的链表长度大于8时，该节点的链表会转换成红黑树（其他的节点还是链表），提高查询效率,而链表长度小于6时又会退化成链表



##### 分散热点

*LongAdder的基本思路就是分散热点*,将value值分散到到一个Call数组中,不同线程会命中到数组的不同槽中,各个线程只对自己槽中的那个值进行CAS操作 

ConcurrentHashMap



##### volatile

```
由于现代操作系统都是多处理器操作系统，每个处理器都会有自己的缓存，可能存再不同处理器缓存不一致的问题，而且由于操作系统可能存在重排序，导致读取到错误的数据，因此，操作系统提供了一些内存屏障以解决这种问题.
简单来说:
1.在不同CPU执行的不同线程对同一个变量的缓存值不同，为了解决这个问题。
2.用volatile可以解决上面的问题，不同硬件对内存屏障的实现方式不一样。java屏蔽掉这些差异，通过jvm生成内存屏障的指令。
对于读屏障:在指令前插入读屏障，可以让高速缓存中的数据失效，强制从主内存取。
```

Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。

## 线程池

参数：corePoolSize - 线程池核心池的大小。 maximumPoolSize - 线程池的最大线程数。 keepAliveTime - 当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间。 unit - keepAliveTime 的时间单位。 workQueue - 用来储存等待执行任务的队列。 threadFactory - 线程工厂。 handler - 拒绝策略。 

用 `Executors` 工具类中提供的方法。因为这些线程池的创建都不够精细化，也非常容易造成OOM风险 ；使用 new ThreadPoolExecutor() 方式创建的线程池是可以通过提供的 set 方法进行动态调整的。有了这个动态调整的方法后，就可以把线程池包装起来，在配合动态调整的页面，动态更新线程池参数，就可以非常方便的调整线程池了 ；

线程监控：常规继承，重写一些方法，记录信息。

**基于JVMTI方式监控**   非入侵代码 

JVMTI(JVM Tool Interface)位于jpda最底层，是Java虚拟机所提供的native编程接口。JVMTI可以提供性能分析、debug、内存管理、线程分析等功能。

基于jvmti提供的接口服务，运用C++代码(win32-add_library)在Agent_OnLoad里开发监控服务，并生成dll文件。开发完成后在 VM vptions 中配置 加入agentpath，这样就可以监控到我们需要的信息内容。

开源的动态可监控线程池框架（**DynamicTp**） ，项目地址：<https://dynamictp.cn/> 

https://www.shuzhiduo.com/A/D854Ay9QdE/  非入侵，配置中心，多平台告警，轻量级（基于 springboot 实现，引入 starter ）

线程池参数动态化 ：https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html

## java内存结构

 JMM https://www.jianshu.com/p/76959115d486

![img](https://upload-images.jianshu.io/upload_images/10006199-a4108d8fb7810a71.jpeg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp) 

程序计数器是一块很小的内存空间，它是线程私有的，可以认作为当前线程的行号指示器。

**每个方法被执行的时候都会创建一个栈帧用于存储局部变量表，操作栈，动态链接，方法出口等信息** 

Java虚拟机栈可能出现两种类型的异常：

1. 线程请求的栈深度大于虚拟机允许的栈深度，将抛出StackOverflowError。
2. 虚拟机栈空间可以动态扩展，当动态扩展是无法申请到足够的空间时，抛出OutOfMemory异常。

 方法区 用于存储已被虚拟机加载的类信息、常量、静态变量，如static修饰的变量加载类的时候就被加载到方法区中。 运行时常量池是方法区的一部分，class文件除了有类的字段、接口、方法等描述信息之外，还有常量池用于存放编译期间生成的各种字面量和符号引用。



```
对象在内存中存储的布局分为 ：
1.对象头 markword
2.实例数据
3.对齐填充
```

![img](https://upload-images.jianshu.io/upload_images/10006199-318ad80ccb29abe4.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp) 

对象头结构

一个类的大小计算

## JVM调优

工具步骤 https://blog.csdn.net/weixin_47184173/article/details/114919061

Sun JDK监控和故障处理命令有top 命令  **jps jstat jmap jhat jstack jinfo** 

Arthas 是Alibaba开源的Java诊断工具。安装在系统所在服务器。可以帮助开发人员或者运维人员查找问题，分析性能，bug追踪。 

MAT的强大之处只要在于对堆与对象的底层分析。在发生内存泄漏，或者OOM的时候 

代码中对线程取了别名，因此可以更快的定位问题线程 

## 类加载

过程 自定义类加载器

- 自己编写的类加载器，需要继承ClassLoader类或URLClassLoader，并至少重写其中的findClass(String
  name)方法，若想打破双亲委托机制，需要重写loadClass方法

- ```
  public abstract class ClassLoader {
    //  每个类加载器都有一个父加载器
    private final ClassLoader parent;
    public Class<?> loadClass(String name) throws ClassNotFoundException {
          return loadClass(name, false);
      }
       protected Class<?> loadClass(String name, boolean resolve)
          throws ClassNotFoundException
      {
              // First, check if the class has already been loaded
              Class<?> c = findLoadedClass(name);
             // 如果没有加载过
              if (c == null) {
                  if (parent != null) {
                    //  先委托给父加载器去加载，注意这是个递归调用
                   c = parent.loadClass(name, false);
                  } else {
                   // 如果父加载器为空，查找 Bootstrap 加载器是不是加载过了
                     c = findBootstrapClassOrNull(name);
                  }
                
              // 如果父加载器没加载成功，调用自己的 findClass 去加载
                  if (c == null) {        
                      c = findClass(name);
                  }
              } 
          
              return c;
          }
          
      }
      //ClassLoader 中findClass方式需要被子类覆盖，下面这段代码就是对应代码
        protected Class<?> findClass(String name){
         //1. 根据传入的类名 name，到在特定目录下去寻找类文件，把.class 文件读入内存
            ...
         //2. 调用 defineClass 将字节数组转成 Class 对象
         return defineClass(buf, off, len)；
      }
        // 将字节码数组解析成一个 Class 对象，用 native 方法实现
      protected final Class<?> defineClass(byte[] b, int off, int len){
      
      }
      
  }
  
  ```

  tomcat打破了双亲委派的原则，实际是在应用类加载器中打破了双亲委派，其他类加载器还是遵循双亲委派的 

  - Catalina ClassLoader ⽤于加载服务器内部可⻅类，这些类应⽤程序不能访问；
  - SharedClassLoader ⽤于加载应⽤程序共享类，这些类服务器不会依赖；
  - WebappClassLoader，每个应⽤程序都会有⼀个独⼀⽆⼆的Webapp ClassLoader，他⽤来加载本应⽤程序 /WEB-INF/classes 和 /WEB-INF/lib 下的类。

  ![img](https://img.jbzj.com/file_images/article/202111/20211118110722158.png?2021101811729) 

https://baijiahao.baidu.com/s?id=1675804610811644518&wfr=spider&for=pc

![img](https://pics1.baidu.com/feed/6f061d950a7b020873653dcada5cd9d4562cc8fd.jpeg@f_auto?token=0c68fa898abec150f15a566e5de4a299) 

加载阶段完成之后，虚拟机就会把外部的二进制字节流（不论从何处获取的）按照一定的数据格式存储在运行时数据区中的方法区。然后在内存中实例化一个java.lang.Class对象 

验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求 

准备阶段是类变量分配内存并设置初始值的阶段。这里的类变量指的是被static修饰的变量，而不包括实例变量。类变量被分配到方法区中，而实例变量存放在堆中。 如果变量被static 和 final同时修饰，则准备阶段直接赋值为指定值。 

解析阶段是将常量池中的符号引用转换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法属性、方法句柄、调用点限定符7类符号引用。 

初始化阶段是执行类构造器 < clinit > 方法的过程。

首先说下类构造器 < clinit > 方法和实例构造器 < init > 方法有什么区别。< clinit > 方法是在类加载的初始化阶段执行，是对静态变量、静态代码块进行的初始化。而< init > 方法是new一个对象，即调用类的 constructor方法时才会执行，是对非静态变量进行的初始化。

使用new关键词创建对象时，访问某个类的静态变量或给静态变量赋值时，调用类的静态方法时。反射调用时，会触发类的初始化（如Class.forName()）初始化一个类的时候，如其父类未初始化，则会先触发父类的初始化。虚拟机启动时，会先初始化主类（即包含main方法的类）。另外，也有些场景并不会触发类的初始化：

通过子类调用父类的静态变量，只会触发父类的初始化，而不会触发子类的初始化（因为，对于静态变量，只有直接定义这个变量的类才会初始化）。通过数组来创建对象不会触发此类的初始化。（如定义一个自定义的Person[] 数组，不会触发Person类的初始化）通过调用静态常量（即static final修饰的变量），并不会触发此类的初始化。因为，在编译阶段，就已经把final修饰的变量放到常量池中了，本质上并没有直接引用到定义常量的类，因此不会触发类的初始化。

## 垃圾回收

内存结构 算法 时机 类型

可达性分析算法帮我们解决了哪些对象可以回收的问题 

垃圾收集算法则关心怎么回收 

```
Java堆是GC回收的“重点区域”。堆中基本存放着所有对象实例，gc进行回收前，第一件事就是确认哪些对象存活，哪些死去[即不可能再被引用]
为了高效的回收，jvm将堆分为三个区域
1.新生代（Young Generation）NewSize和MaxNewSize分别可以控制年轻代的初始大小和最大的大小
2.老年代（Old Generation）
3.永久代（Permanent Generation）【1.8以后采用元空间，就不在堆中了】

可作为GC Roots的对象有四种
①虚拟机栈(栈桢中的本地变量表)中的引用的对象。
②方法区中的类静态属性引用的对象，一般指被static修饰引用的对象，加载类的时候就加载到内存中。
③方法区中的常量引用的对象,
④本地方法栈中JNI（native方法)引用的对象
要真正宣告对象死亡需经过两个过程。
1.可达性分析后没有发现引用链
2.查看对象是否有finalize方法，如果有重写且在方法内完成自救[比如再建立引用]，还是可以抢救一下，注意这边一个类的finalize只执行一次，这就会出现一样的代码第一次自救成功第二次失败的情况。[如果类重写finalize且还没调用过，会将这个对象放到一个叫做F-Queue的序列里，这边finalize不承诺一定会执行，这么做是因为如果里面死循环的话可能会时F-Queue队列处于等待，严重会导致内存崩溃，这是我们不希望看到的。]
GC是怎么判断对象是被标记的？通过枚举根节点的方式，通过jvm提供的一种oopMap的数据结构
```

![img](https://upload-images.jianshu.io/upload_images/10006199-854e1de91f66764b.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp) 

```
1.标记/清除算法【最基础】
2.复制算法
3.标记/整理算法
jvm采用`分代收集算法`对不同区域采用不同的回收算法。
新生代采用复制算法：将内存分为一块Eden空间和From Survivor、To Survivor【保留空间】，三者默认比例为8:1:1，优先使用Eden区，若Eden区满，则将对象复制到第二块内存区上。但是不能保证每次回收都只有不多于10%的对象存货，所以Survivor区不够的话，则会依赖老年代年存进行分配
 GC开始时，对象只会存于Eden和From Survivor区域，To Survivor【保留空间】为空。
GC进行时，Eden区所有存活的对象都被复制到To Survivor区，而From Survivor区中，仍存活的对象会根据它们的年龄值决定去向，年龄值达到年龄阈值(默认15是因为对象头中年龄战4bit，新生代每熬过一次垃圾回收，年龄+1)，则移到老年代，没有达到则复制到To Survivor。
老年代采用标记/清除算法或标记/整理算法
由于老年代存活率高，没有额外空间给他做担保，必须使用这两种算法。
```

G1(garbage first:尽可能多收垃圾，避免full gc)收集器是当前最为前沿的收集器之一(1.7以后才开始有)，同cms一样也是关注降低延迟，是用于替代cms（CMS采用的是"标记-清除"(Mark Sweep)算法，而且是支持并发(Concurrent)的 ）功能更为强大的新型收集器，因为它解决了cms产生空间碎片等一系列缺陷。

 g1通过并发(并行)标记阶段查找老年代存活对象，通过并行复制压缩存活对象【这样可以省出连续空间供大对象使用】。

g1将一组或多组区域中存活对象以增量并行的方式复制到不同区域进行压缩，从而减少堆碎片，目标是尽可能多回收堆空间【垃圾优先】，且尽可能不超出暂停目标以达到低延迟的目的。

 Minor GC、Major GC、FULL GC、mixed gc

stop the world简单来说就是gc的时候，停掉除gc外的java线程。 

新生代什么样的情况会晋升为老年代？

对象优先分配在eden区，eden区满时会触发一次minor GC

> 对象晋升规则
>  1 长期存活的对象进入老年代，对象每熬过一次GC年龄+1(默认年龄阈值15，可配置)。
>  2 对象太大新生代无法容纳则会分配到老年代
>  3 eden区满了，进行minor gc后，eden和一个survivor区仍然存活的对象无法放到(to survivor区)则会通过分配担保机制放到老年代，这种情况一般是minor gc后新生代存活的对象太多。
>  4 动态年龄判定，为了使内存分配更灵活，jvm不一定要求对象年龄达到MaxTenuringThreshold(15)才晋升为老年代，若survior区相同年龄对象总大小大于survior区空间的一半，则大于等于这个年龄的对象将会在minor gc时移到老年代

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy83eUdpYlRNTE1oWFBmZ0s0aWFpYmNpYlJtZ1NOTVA3SVdBRmliaWNKN3k5amZqMlk0S1hxZ1A2M3V6aWNIamRHWlA3dXZrSVo2SUtERkZMeVJpYnNJSHVJQkZRZ2VBLzY0MA?x-oss-process=image/format,png) 

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy83eUdpYlRNTE1oWFBmZ0s0aWFpYmNpYlJtZ1NOTVA3SVdBRmliWmtSZU9PY3FXSVgxdkZ0RkJaeHI4bEE5U0ZqWVlwRzV5Q0Z1cG90bHZnZzVpY2xjRmljdjJqWUEvNjQw?x-oss-process=image/format,png) 

```
-Xms20m -Xmx20m -Xmn10m -XX:SurvivorRatio=8 -XX:+PrintGCDetails
```

##  内存泄漏OOM

现象：1.oom、2.有些场景下还会看到频繁执行full GC ，垃圾回收后，内存并没有减少多少 3.性能下降，莫名崩溃，接口调用失败

jconsole：本地启动后查看内存增长情况，点击执行GC后发现内存没有回收

idea连接方式：添加vm配置，然后启动main方法后，启动jconsole用远程连接127.0.0.1:8888,

```
-Djava.rmi.server.hostname=127.0.0.1
-Dcom.sun.management.jmxremote
-Dcom.sun.management.jmxremote.port=8888
-Dcom.sun.management.jmxremote.ssl=false
-Dcom.sun.management.jmxremote.authenticate=false
```

生产问题可以分析dump文件

可以使用MAT分析堆转储dump文件 

思路：查找哪个对象的个数和占有空间再持续上升，特别是还发生过GC以后；查询是否有大量的线程出现

IDEA中内存分析工具—JProfiler插件

场景：

1.静态属性导致内存泄露
2.未关闭的资源
无论什么时候当我们创建一个连接或打开一个流，JVM都会分配内存给这些资源。比如，数据库链接、输入流和session对象。
如果进行处理呢?第一，始终记得在finally中进行资源的关闭;第二，关闭连接的自身代码不能发生异常;第三，Java7以上版本可使用try-with-resources代码方式进行资源关闭。
3.不当的equals方法和hashCode方法实现
4.使用ThreadLocal
第一，使用ThreadLocal提供的remove方法，可对当前线程中的value值进行移除;
第二，不要使用ThreadLocal.set(null) 的方式清除value，它实际上并没有清除值，而是查找与当前线程关联的Map并将键值对分别设置为当前线程和null。
第三，最好将ThreadLocal视为需要在finally块中关闭的资源，以确保即使在发生异常的情况下也始终关闭该资源。
5.finalize()方法,并且重写的方法在执行时需要一些时间。
6.外部类引用内部类;如果内部类不需要访问外部类的成员信息，可以考虑将其转换为静态内部类。

7.大量线程不能结束导致，比如FutureTask，线程池拒绝策略没有抛异常，后边用到FutureTask.get一直阻塞。

再比如，线程池里的任务需要写日志，但是零点的时候服务器日志切割，导致阻塞，导致堆积线程

大量日志时，从Blocked线程堆栈着手分析，查看PrintStream相关代码 

```
  public void println(String x) {
        synchronized (this) {
            print(x);
            newLine();
        }
    }
```

1.日志量过大导致AsyncAppender日志队列被打满，新的日志事件无法入队，进而由ErrorHandler处理日志，同时由于ErrorHandler存在线程安全问题，导致大量日志输出到了Console，而Console在输出日志到PrintStream输出流时，存在synchronized同步代码块，所以在高并发场景下导致线程Block。 

2.Log4j2打印异常日志时，AsyncLoggerConfig会初始化Disruptor RingBuffer日志元素字段，并进一步触发解析、加载异常堆栈类。JVM通过生成字节码的方式优化反射调用性能，但该动态生成的类无法被WebAppClassLoader类加载器加载，因此当大量包含反射调用的异常堆栈被输出到日志时，会频繁地触发类加载，由于类加载过程是synchronized同步加锁的，且每次加载都需要读取文件，速度较慢，从而导致线程Block。 

4种报错：

java heap space, PermGen space ,unable to create new native thread, GC overhead limt exceeded.

##  springboot

#### 自动装配原理

```
自动装配原理：通过3个注解
@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),
		@Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })
public @interface SpringBootApplication {

@SpringBootConfiguration 注解标记启动类为配置类
@ComponentScan 注解实现启动时扫描启动类所在的包以及子包下所有标记为bean的类由IOC容器注册为bean
@EnableAutoConfiguration通过 @Import 注解导入 AutoConfigurationImportSelector类，然后通过AutoConfigurationImportSelector 类的 selectImports 方法去读取需要被自动装配的组件依赖下的spring.factories文件配置的组件的类全名，并按照一定的规则过滤掉不符合要求的组件的类全名，将剩余读取到的各个组件的类全名集合返回给IOC容器并将这些组件注册为bean
————————————————
@Import(AutoConfigurationImportSelector.class)

```

#### 事务不回滚的原因

1.该方法非public方法， spring aop中动态代理时有public的检查

2.被try catch住，未抛出异常的情景

3.非事务方法里调用事务方法， 也是没法调用到动态代理对象

4.抛出检查异常的情况，必须抛出runtimeException或error



## 微信小程序开发

注册、微信IDE前端开发、后端服务部署（常规方法，阿里云等上部署一个web服务）、服务URL配置到微信管理台里、发布。s

## 基本算法

### 五大算法

分治法
动态规划：

0-1背包问题说的是，给定背包容量W，一系列物品{weiht,value}，每个物品只能取一件，计算可以获得的value的最大值。最优解问题，当然是我们DP，最难的一步还是状态转移方程 ；

**状态转移方程**就是取放进去和不放进去两种情况的最大值"m【i】【j】 = max{ m【i-1】【j-w【i】】+v【i】 , m【i-1】【j】}" 

贪心法
回溯法
分支限界法

### 位图：

https://blog.csdn.net/qq_40100414/article/details/118719748

### 位运算：

a*33=a*32+a*1=a*2^5+a*2^0=a<<5+a<<0;

位运算符	解释
lowbit(i) 即i & -i	返回i的最后一位1
n>>k & 1	求n的第k位数字
x | (1 << k)	将x第k位 置为1
x ^ (1 << k)	将x第k位取反
x & (x - 1)	将x最右边的1置为0(去掉最右边的1)
x | (x + 1)	将x最右边的0置为1
x & 1	判断奇偶性 真为奇，假为偶
————————————————

原码：**是最简单的机器数表示法，用最高位表示符号位，其他位存放该数的二进制的绝对值**。 

反码：**正数的反码还是等于原码；负数的反码就是它的原码除符号位外，按位取反**。 

 计算机采用补码：正数的补码等于它的原码；负数的补码等于反码+1 

###  二分搜索：

```
40亿个随机生产的32位整数，求出一个缺失的数：二进制01二分后，从少的一组里找
```

## 设计模式

23种设计模式代码及jdk或其他实际的示例。

![img](https://images2017.cnblogs.com/blog/401339/201709/401339-20170928225241215-295252070.png) 

todo：实际项目里的应用

## GIT

#### 一台电脑（终端）可以配置多个 SSH-Key 用于多个 Git 账号。

换个名称，加个config。 

```
1.ssh-keygen -t rsa -C '[邮箱]' -f ~/.ssh/github_id_rsa
ssh-keygen -t rsa -C '[邮箱]' -f ~/.ssh/gitlab_id_rsa
2.新建config并配置： touch config 、vim config，内容如下：
# github
Host github.com
HostName github.com
PreferredAuthentications publickey
IdentityFile ~/.ssh/github_id_rsa
# gitlab
Host gitlab.com
HostName gitlab.com
PreferredAuthentications publickey
IdentityFile ~/.ssh/gitlab_id_rsa
# 如果生成多个 SSH-Key , 则按上面的格式继续往下写
3.前往 ~/.ssh/ 目录下查看生成的文件：cat [xxx]_rsa.pub
4. 在 Github 或其他 Git 服务器新建 SSH Key，输入文本即可。
```

sourcetree：指定忽略文件，设置->高级:

```
*.iws
*.iml
*.ipr
target/
.settings
.project
.classpath
.externalToolBuilders
*.class
*svn/
.idea/
*.jar
.gitignores
~*
```



#### git常用命令

git clone ***

git push origin master



#### idea破解

安装插件重启即可

plugin：IDE Eval Reset

server：https://plugins.zhile.io  

如果搜索不到，则设置 http proxy—>勾上Auto-detect proxy setting,加上地址 http://127.0.0.1:1080 

## Redis

#### 作用

1. 缓存

2. 数据共享分布式，如应用共享session

3. 全局ID

   int类型，incrby，利用原子性

   `incrby userid 1000`

   [分库分表](https://so.csdn.net/so/search?q=%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8&spm=1001.2101.3001.7020)的场景，一次性拿一段

   5、计数器

   int类型，incr方法

   例如：文章的阅读量、微博点赞数、允许一定的延迟，先写入Redis再定时同步到数据库

   计数功能应该是最适合 Redis 的使用场景之一了，因为它高频率读写的特征可以完全发挥 Redis 作为内存数据库的高效。在 Redis 的数据结构中，string、hash和sorted set都提供了incr方法用于原子性的自增操作，下面举例说明一下它们各自的使用场景：

   如果应用需要显示每天的注册用户数，便可以使用string作为计数器，设定一个名为REGISTERED_COUNT_TODAY的 key，并在初始化时给它设置一个到凌晨 0 点的过期时间，每当用户注册成功后便使用incr命令使该 key 增长 1，同时当每天凌晨 0 点后，这个计数器都会因为 key 过期使值清零。
   每条微博都有点赞数、评论数、转发数和浏览数四条属性，这时用hash进行计数会更好，将该计数器的 key 设为weibo:weibo_id，hash的 field 为like_number、comment_number、forward_number和view_number，在对应操作后通过hincrby使hash 中的 field 自增。
   如果应用有一个发帖排行榜的功能，便选择sorted set吧，将集合的 key 设为POST_RANK。当用户发帖后，使用zincrby将该用户 id 的 score 增长 1。sorted set会重新进行排序，用户所在排行榜的位置也就会得到实时的更新。
   ————————————————
   原文链接：https://blog.csdn.net/m0_67393593/article/details/125241392

   **点赞、签到、打卡**

   假如上面的微博ID是t1001，用户ID是u3001

   用 like:t1001 来维护 t1001 这条微博的所有点赞用户

   点赞了这条微博：sadd like:t1001 u3001
   取消点赞：srem like:t1001 u3001
   是否点赞：sismember like:t1001 u3001
   点赞的所有用户：smembers like:t1001
   点赞数：scard like:t1001

   **好友关系、用户关注**

   通过`set`解决 交集、差集问题

   ```
   举例
   follow 关注 fans 粉丝
   
   相互关注：
   
   sadd 1:follow 2
   sadd 2:fans 1
   sadd 1:fans 2
   sadd 2:follow 1
   我关注的人也关注了他(取交集)：
   
   sinter 1:follow 2:fans
   可能认识的人：
   
   用户1可能认识的人(差集)：sdiff 2:follow 1:follow
   用户2可能认识的人：sdiff 1:follow 2:follow 
   ```

   **维护商品标签，类似set**

   - sadd tags:i5001 画面清晰细腻；  sadd tags:i5001 真彩清晰显示屏

   - ```
     SMEMBERS somekey  #查看所有元素
     ```

4. 分布式锁：set lock_key locked NX EX 1  ，如果这个操作返回false，说明 key 的添加不成功，也就是当前有人在占用这把锁。而如果返回true，则说明得了锁，便可以继续进行操作，并且在操作后通过del命令释放掉锁。并且即使程序因为某些原因并没有释放锁，由于设置了过期时间，该锁也会在 1 秒后自动释放，不会影响到其他程序的运行。 

   推荐使用 redisson 第三方库实现分布式锁。 

   https://blog.csdn.net/agonie201218/article/details/121423212





1.KEYS pattern 查找所有匹配给定的模式的键,keys * 查看所有缓存的键

2.DEL key1 key2 删除指定的缓存(一个或多个)，get key //读取  set key value // 写入 

3.自增：

+ incr key // 键值加1 + decr key // 键值减1 + incrby key amount // 键值加amount + decrby key amount // 键值减amount + incrbyfloat key amount // 键值加上浮点数amount 

4.二进制位：

+ setbit key offset value //把指定的bit位设置为value(value只能是0或1) 
+ getbit key offset //获取指定bit位的二进制的值 

#### 面试题

1.基本场景都是单线程，4.0后添加了多线程的支持，主要是在大数据的异步删除功能上

2.单线程为什么也快？基本在内存中完成操作，高效的数据结构哈希表、跳表；

单线程避免多线程的竞争、采用I/O多路复用机制（非阻塞）处理大量客户端的socket请求

3.如何实现数据不丢失？数据持久化

AOF日志，文件追加方式，记录所有的操作命令，写后日志（写日志不检查命令正确性，所以放后边），先命令后日志，恢复时需要全量执行一遍；会阻塞线程

RDB快照，将某个时刻的内存数据二进制写入磁盘

4.0后增加了两种方式的混合持久化方式

4.如何实现高可用

A。主从复制

B。哨兵模式

C。集群模式：为了扩展写能力和储存能力，因为AB只能提高读并发。

集群中多master节点，如何存储选取？一致性hash算法。

5.过期策略？内存淘汰策略？

过期删除策略，是删除已过期的 key，而当 Redis 的运行内存已经超过 Redis 设置的最大内存之后，则会使用内存淘汰策略删除符合条件的 key，以此来保障 Redis 高效的运行。 

过期删除策略：**惰性删除+定期删除** 

惰性删除是访问key时检查是否超期并删除。定期删除是**每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。** 

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5/%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5.jpg) 

**在 LRU 算法中（** Least Recently Used 翻译为**最近最少使用** **）**，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。

**在 LFU 算法中**Least Frequently Used 翻译为**最近最不常用的** ，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，低 8bit 存储 logc(Logistic Counter)。logc 并不是单纯的访问次数，而是访问频次（访问频率），因为 **logc 会随时间推移而衰减的**。 

## 并发编程源码分析

syncronize 底层原理

volitile 底层原理

锁：公平锁和非公平锁实现原理？可重入锁实现原理？共享锁和排他锁的实现原理？

AQS？state机制

线程池

FutureTask、ExecutorCompletionService

线程通信

阻塞队列

工具类：信号量、降锁、栅栏、



### 队列

LinkedBlockingQueue中的锁是分离的，生产者的锁PutLock，消费者的锁takeLock，提高了线程并发执行的效率 ；内部维护的是一个链表结构 ；一般使用指定长度，若设置为无界队列，队列自身占用很大的内存，容易出现OOM 

而ArrayBlockingQueue生产者和消费者使用的是同一把锁；内部维护了一个数组 

相同点：都是通过ReentrantLock及condition通知机制来实现可阻塞式插入和删除元素并满足线程安全的特性； 

默认非公平take，公平队列也可以实现方式：lock = new ReentrantLock(fair);

### 应用场景

1. 报表 批量任务，多线程并行执行
2. 线程池的队列做限流
3. 阻塞队列应用再日志系统，生产者消费者模式

### 并发的三种场景

#### 分工

多线程并发最基本的场景就是分工。分工，就是线程各司其职，完成不同的工作。分工，也是有很多模式的。比如有:

- 生产者-消费者模式，大厨做饭，我们来吃；
- MapReduce模式，把工作拆分成多份，多个线程共同完成后，再组合结果，Java8中的stream与Fork/Join就是这种模式的体现；
- Thread-Per-Message模式，服务端就是这种模式，收到消息给不同的Thread进行处理；
- ... ...

#### 同步

有分工就要有同步，不同工人之间要协作，不同线程也是。一个线程的执行条件往往依赖于另一线程的执行结果。

线程之间最基本的通信机制是管程模式与wait/notify,除此外还有多个工具类，如：

- Future及其衍生的工具类FutureTask/CompletableFuture等，可以完成异步编程；

- CountDownLatch/CyclicBarrier可以实现特定场景的协作；

- Semaphore提供了经典的PV同步原语（p操作和v操作是不可中断的程序段，称为原语。P,V原语中P是荷兰语的Passeren，相当于英文的pass, V是荷兰语的Verhoog,相当于英文中的incremnet。 

  还可以作为限流器使用；

- ReentrantLock与Condtion,对管程同步的扩展；

- ... ...

#### 互斥

不同工人，操作相同的共享资源，就有可能冲突。类似，多线程访问相同的共享变量，就需要做互斥处理。分工与协作强调的是性能，互斥问题强调的是正确，即线程安全问题。Java解决互斥问题提供了很多思路与工具。

- 避免共享，没有共享，没有竞态，就没有伤害，如ThreadLocal；
- 没有改变，如果大家都不做改变，都是只读的，一起也没有错；
- Copy-on-write，你变你的，我变我的，每变一次都生成新的副本，只要不冲突就可以并行；
- CAS，写入前要看一看，有没有物逝人非（变量和自己读取时一样），没有再写入，否则再做一变；
- Lock，最终手段，但也不想做得太绝，够用就行，ReadWriteLock/StampedLock，够用就行；

互斥问题很像数据库，都是一样的。天下的互斥都是一样的。

## jenkins

jenkins+git+maven搭建自动化部署环境 

## JavaScript 

闭包

模块化

## AC架构



## 面试题库

##### 1.跳表的数据结构

![img](https://img-blog.csdnimg.cn/20181029211746472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Bjd2wxMjA2,size_16,color_FFFFFF,t_70) 

**跳表在原有的有序链表上增加了多级索引，通过索引来实现快速查询。跳表不仅能提高搜索性能，同时也可以提高插入和删除操作的性能。** 每一级索引都最多只需要遍历3个结点。

因此，m=3，所以**跳表查找任意数据的时间复杂度为O(logn)**，这个查找的时间复杂度和[二分查找](https://so.csdn.net/so/search?q=%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE&spm=1001.2101.3001.7020)是一样的，但是我们却是基于单链表这种数据结构实现的。

ConcurrentSkipListMap  ConcurrentSkipListSet 

##### 2.lock与syncronized区别，lock的实现，读写锁的实现，锁升级，锁降级

![img](https://pic3.zhimg.com/80/v2-978013a2b71f7670d3036a496fb4b86a_720w.webp) 

1.synchronized是关键字,Lock是接口;
2.synchronized是隐式的加锁,lock是显式的加锁;
3.synchronized可以作用于方法上,lock只能作用于方法块;
4.synchronized底层采用的是objectMonitor,lock采用的AQS;
5.synchronized是阻塞式加锁,lock是非阻塞式加锁支持可中断式加锁,支持超时时间的加锁;
6.synchronized在进行加锁解锁时,只有一个同步队列和一个等待队列, lock有一个同步队列,可以有多个等待队列;
7.synchronized只支持非公平锁,lock支持非公平锁和公平锁;
8.线程通信：synchronized使用了object类的wait和notify进行等待和唤醒, lock使用了condition接口进行等待和唤醒(await和signal);且可以多路等待
9.与synchronized 不同的是，一旦synchronized 块结束，就会自动释放对someObject的占用。 lock却必须调用unlock方法进行手动释放，
为了保证释放的执行，往往会把unlock() 放在finally中进行；

![img](https://img-blog.csdnimg.cn/bef8b582ef52416b9926856f101b6b1e.png) 

锁升级是synchronized关键字在jdk1.6之后做的优化，锁升级的顺序为：

无锁 -> 偏向锁 -> 轻量级锁 -> 重量级锁，且锁升级的顺序是不可逆的。

锁降级是为了保证数据的可见性在添加了写锁后再添加一道读锁 





##### 3.mysql 优化大偏移量的性能 

通过延迟关联降低扫描的页面（数据列）
select * from (select id from t limit 100000, 10) tmp inner join t using(id);
上面的语句通过先扫描出对应的主键，然后再回表查询出对应的列，极大的减少了MySQL对数据页的扫描。

预先计算分页所在范围
前提条件是在某个有索引的数据列上可以通过计算出对应分页的范围值，且只可根据该索引列排序。
 select * from t_log where id > #{pre_max_id} limit 20; 

##### 4.Java终止线程的方式

**最正确**的停止线程的方式是使用 `interrupt`。 但 interrupt仅仅起到**通知**被停止线程的作用。 而对于被停止的线程而言，它拥有完全的**自主权**，它既可以选择立即停止，也可以选择一段时间后停止，也可以选择不停止。



## 英文自我介绍

早上好，很高兴能得到这次面试机会。

我将用几分钟时间介绍我自己，让你全面了解我的能力，我的观点，我的项目经历。

我一直从事软件开发相关工作，将近15年；我了解整个软件开发的全过程，需求分析、系统设计、数据库设计、编码、测试、运维等。我有丰富的项目经历，做过10多个大小项目，大部分是web开发、也有app开发。

在大部分项目里，我都是项目骨干，开发骨干、项目组长并成功交付项目。能做到这样，我认为最关键的一点就是设计，可以是系统设计，也可以是一个功能的设计，我总是保证设计先行。做好了设计，后面无论用什么编程语言实现都可以，我所做的项目里也有各种编程语言，有java、javascript、ios object-c、vb.net，这很好的证明了我的观点。

能做好设计，关键的能力是逻辑能力，全面的考虑问题；我认为我擅长做逻辑思考，然后给出完整的解决方案。因此我可以做需求、做设计、做编码、甚至测试。

最后个人兴趣方面，我喜欢的运动是篮球，基本保持一周两次的节奏。适量的运动有助于身心健康，而健康是一切幸福的基础。

我将持续运动下去，也希望结识新的球友。

谢谢，再次感谢给与这个机会



Good morning. I'm glad to have this interview.

I'll take a few minutes to introduce myself.

I have been working on software development for nearly 15 years. I have experience of  the entire software development process, including requirements analysis, system design, database design, coding, testing, operation and maintenance. and have worked on more than 10 projects, mostly web development and app development.

In most projects, I am the *key member* or the team leader and delivering the project successfully; why succeed？ I think the most *important*  thing is the design . I always guarantee that the design is finished before the programming. After designed, no matter what programming language you can implement it ； there are also a variety of programming languages used in my past projects, such as java, javascript, IOS object-c, vb.net, which is a good proof of my point of view.

The key ability to make a good design is the logical ability  ; I think I'm good at logically thinking  and always*propose*    a complete solution. So I can do requirements, design, coding, and even testing.

At last,about My hobbies ，my favorite sport is basketball,  basically twice a week. *The exercise is helpful to*  physical and mental health, which is the basis of all happiness.

I will keep moving .

That‘s all, thank you again for this opportunity.

During holidays, I usually play basketball with my colleagues or climb mountains  with my family.

I'm sorry. I don't follow you. would you please say it again slowly?

一个项目经历：

我要介绍的项目，它是一个医疗企业针对新产品引入中国市场的过程管理系统，（**New Product Launch ）叫NPL；这个过程是一个长达数年、经历十多项目任务、多团队协作的一个过程， NPL项目以任务管理的方式，将新产品拿证前各团队任务进行系统化记录、追踪，使得新产品上市流程透明化，加强团队间协同，统一管理各类注册用文件，加速新产品上市流程

系统采用微服务架构 Spring Cloud 

我作为唯一后端开发，工作内部包括业务设计、系统设计、DB设计及编码实现；我开发了所有后端工作，前端由另一个同时完成。

这是一个小项目，但是所有的软件开发过程都需要我一个人完成。这很锻炼人。

The project I want to introduce is the process management system of a medical enterprise for the introduction of new products into the Chinese market,  we called NPL ,  short for "New Product Launch"; 

the process of New Product Launch mabe take seveval years,and includes more than ten subtasks,

NPL system can manage the tasks, record  and track the process ,also reminds the task owner to finish it;  there is a dashbord,The department manager can see the overall situation of the new product launch in the next few years

The system uses Spring Cloud  micro-service architecture, database is sqlserver,and deplyment in microsoft cloud.

I am the only backend developer ,so my job include business design, system design, Database design ,coding and testing.

Although  This is a small project,  all software development processes must be completed  by myself. This is really a challenge for me.  Finally, the project was successfully completed.

 knowledge transfer*(KT)* 

## 公司准备

「Veeva」维我软件

Veeva是一家美国基于SaaS模式的医疗CRM系统提供商
生命科学 Life Sciences ：**2024年生命科学市场规模达到约800亿元的市场规模** 

*SaaS*（Software as a Service) :

系统集成

*CRM*（Customer Relationship Management） 的价值在于突出销售管理、营销管理、客户服务与支持方面的重要性 

简历体现：

2.5w

为什么来我们公司？

过去的经历主要是项目型，各种行业项目都有所接触，但都没有深入的积累与沉淀；

后面的职位规划上，更着重某一行业的积累。

维我软件专注于生命科学领域的解决方案，中国老龄化越来越严重，生命科学领域是未来具有潜力的领域，大有作为。

问题：

大连分公司的业务在现在及未来的定位或分工或地位是什么样？

对出差有没有要求？

薪酬福利的架构简单介绍一下？

微信开发



todo: 把场景加入项目

1. oom ：
2. 消息队列 ：日志打印、邮件发送解耦  rocketmq 代码试一下
3. redis：数据缓存
4. 分布式锁:  集群部署 定时任务重复问题

## 工作中遇到的问题

1. 难题，如何解决

   最近一个项目，因为有个excel导入的需求，里面有7个sheet页面，每个字段都有几十个，导入的字段需要做各种校验，长度、必填，数据类型、日期校验、字典校验，校验不符合还得输出错误提示“哪个字段什么不符合”，工期只有1个星期，如果按照以往每个字段单独写一个方法去做校验，时间不太够，调试也不方便；

   不写死代码就必须循环自动化方式去处理，首先需要配置这些信息，哪个字段需要做什么校验；

   因为excel导入有个对应的model，上面已经配置了不少信息，如中文名，是否必填，所有我就想到用注解，开发一个注解，然后字段上配置上需要的信息。excel导入后，先解析出model上的注解信息，循环对每个校验即可。这个方案是个通用方案，所有java的excel导入可以直接拿去用。

   ios btproxy需求的开发：难点在于技术领域不熟悉，包括tcp udpsocket的调用，定制的蓝牙报文协议与蓝牙表做数据交互，报文的分包合并，这过程的问题调试。

2. oom

   开发中遇到,比如  递归调用时发生过栈内存溢出

   -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/temp/dump 

3. 设计模式

   策略模式:  推荐算法实现时，每个条件维度都是一个算法类，然后组合所有选定的算法，依次传入内容类调用处理

   单例： 公积金后台的数据发送接受解析器，就是一个单例。

   模板方法:

   责任链:  hr系统，人员的初始工资导入时，工资的各种校验，比如岗位工资，会根据你的级别，你的职务、工龄，男女不一样算法不一样。

   每种校验一个类，所有的校验配置起来，依次校验接口。而且这种校验后期会随政策不断增加或淘汰

4. 你的缺点

   太理性，太求稳，处事不够圆滑。

   比如客户提了一个需求只给1个月工作量，pm找我做，我评估后要2个月，1个月完不成。然后pm找了B，B却一口答应下来，虽然结果往往是当期交付不了，当中各种问题冒出，最后通过加班加人完成，但在pm眼里，B更加优秀，看到了B各种加班各种解决问题，解决了他的问题。而对我的看法呢，虽然每个迭代如期完成，

   但好像没有尽全力似的，因为我基本不会去找pm反应各种突发问题，我已经在设计阶段考虑到了各种问题并按计划如期进行。结果就是年底加薪B第一我第二。也许我需要做个折中。

5. 你的优点

   除了专业性外，这是基本的要做到的；我觉得我最大的优点还是“执行力”，对上级分配的任务，不打折扣的落实下去，执行好。

6. 其他观点

   对敏捷的思考：整个项目走敏捷管理，每个迭代里我建议按瀑布模式开发。瀑布的优点是比较稳，一步一步走。比如需求设计编码测试。我们要保证每个步骤都要有，尤其是设计，往往很多敏捷项目团队被忽略。

   设计统一了需求方、开发人员、测试人员多方的意见，减少后期的理解误差与沟通成本。设计可以提前识别问题与风险。当然设计也可以按敏捷的模式运作，比如设计的形式、文档可以轻量级，设计可以与编码等其他阶段并行，设计一部分开发一部分，避免等待浪费工作量。







## IDEA快捷键

返回：ctl+alt+ 左箭头